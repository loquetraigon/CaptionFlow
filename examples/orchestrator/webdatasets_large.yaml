# Caption Flow Configuration

orchestrator:
  host: 0.0.0.0
  port: 8765
  #ssl:
  #  # Use the paths output by 'caption-flow generate-cert' command
  #  cert: /etc/letsencrypt/live/caption.example.com/fullchain.pem  # From generate-cert output
  #  key: /etc/letsencrypt/live/caption.example.com/privkey.pem      # From generate-cert output

  # Items per chunk. Having a larger chunk size reduces memory overhead on the orchestrator.
  #  If the chunk size is too large, it can lead to uneven work distribution, especially at the end.
  #  The ideal value depends on the overall size of the dataset, with million+ item datasets benefiting from 10000+
  chunk_size: 1000
  # Chunks sent to worker at once. If too many are sent, it will unevenly distribute work.
  #  If your model is very fast and your images are very small, you may want to increase this to preload data.
  chunks_per_request: 1
  # Target chunks = workers * multiplier. Dynamically increases buffer for additional workers.
  chunk_buffer_multiplier: 2
  # Minimum chunks to keep ready
  min_chunk_buffer: 10
  dataset:
    mock_results: false
    type: "huggingface"
    processor_type: "webdataset"
    dataset_path: "NebulaeWis/e621-2024-webp-4Mpixel" # HF dataset path or local directory
    name: "e621"
    version: "1.0"
    buffer_size: 35

  # vLLM configuration - distributed to all workers
  vllm:
    # Model settings
    mock_results: false
    model: "Qwen/Qwen2.5-VL-7B-Instruct"
    tensor_parallel_size: 1
    max_model_len: 16384
    dtype: "float16" # or "float32", "bfloat16"
    gpu_memory_utilization: 0.92
    enforce_eager: true
    disable_mm_preprocessor_cache: true
    limit_mm_per_prompt:
      image: 1

    # Batching
    batch_size: 64

    # Sampling parameters
    sampling:
      temperature: 0.7
      top_p: 0.95
      max_tokens: 256
      repetition_penalty: 1.05
      skip_special_tokens: true
      stop:
        - "<|end|>"
        - "<|endoftext|>"
        - "<|im_end|>"

    # Inference prompts - each image gets captions from all prompts
      # Add more prompts for diverse captions
    inference_prompts:
      - "without repeating the prompt, complete the sentence: this image shows..."

  storage:
    data_dir: ./caption_data
    checkpoint_interval: 2000
    caption_buffer_size: 10000
    job_buffer_size: 100
    contributor_buffer_size: 10
    checkpoint_dir: ./checkpoints

  auth:
    worker_tokens:
      - token: "worker-alpha-2024"
        name: "NVIDIA 4090 1"
      - token: "worker-beta-2024"
        name: "NVIDIA 4090 2"
      - token: "worker-kappa-2024"
        name: "NVIDIA 4090 3"
      - token: "example-worker-token"
        name: "Example Worker Token"
    monitor_tokens:
      - token: "letmein"
        name: "Default demo Monitor user"
    admin_tokens:
      - token: "admin-secret-2024"
        name: "Default admin user, misconfigured system"
