# Caption Flow Configuration

orchestrator:
  host: 0.0.0.0
  port: 8765
  #ssl:
  #  # Use the paths output by 'caption-flow generate-cert' command
  #  cert: /etc/letsencrypt/live/caption.example.com/fullchain.pem  # From generate-cert output
  #  key: /etc/letsencrypt/live/caption.example.com/privkey.pem      # From generate-cert output

  # Dataset configuration - centrally managed by orchestrator
  chunk_size: 1000 # Items per chunk
  chunks_per_request: 2 # Chunks sent to worker at once
  chunk_buffer_multiplier: 3 # Target chunks = workers * multiplier
  min_chunk_buffer: 10 # Minimum chunks to keep ready
  dataset:
    type: "huggingface"
    processor_type: "huggingface_webdataset"
    dataset_path: "RareConcepts/pixelvision-670k" # HF dataset path or local directory
    name: "pixelvision"
    version: "1.0"
    split: "images"
    dataset_url_column: "full_image_url"

  # vLLM configuration - distributed to all workers
  vllm:
    # Model settings
    model: "Qwen/Qwen2.5-VL-3B-Instruct"
    tensor_parallel_size: 1
    max_model_len: 16384
    dtype: "float16" # or "float32", "bfloat16"
    gpu_memory_utilization: 0.92
    enforce_eager: true
    disable_mm_preprocessor_cache: true
    limit_mm_per_prompt:
      image: 1

    # Batching
    batch_size: 8

    # Sampling parameters
    sampling:
      temperature: 0.7
      top_p: 0.95
      max_tokens: 256
      repetition_penalty: 1.05
      skip_special_tokens: true
      stop:
        - "<|end|>"
        - "<|endoftext|>"
        - "<|im_end|>"

    # With multi-turn inference, each stage can have its own (or a shared) model, and prompt.
    stages:
      - name: "base_caption"
        model: "Qwen/Qwen2.5-VL-3B-Instruct"
        prompts: 
          - "describe this image in detail"
        # The field in the dataset where this caption will go, which will be available to subsequent stages.
        output_field: "caption"
      
      - name: "caption_shortening"
        model: "Qwen/Qwen2.5-VL-7B-Instruct"  # Different model
        prompts: 
          # The value {caption} will expand from the first stage, base_caption.
          - "Please condense this elaborate caption to ONLY the important details: {caption}"
        # This caption will be in its own field called "condensed_caption"
        output_field: "condensed_caption"
        # This stage requires the output from the base_caption stage, providing the {caption} value.
        requires: ["base_caption"]
        gpu_memory_utilization: 0.35  # Stage-specific override to use less memory for this model's KV cache.
      
      - name: "tag_variant"
        prompts:  # Uses default model from vllm config
          - "Please convert this caption to booru tag format instead: {condensed_caption}"
        output_field: "caption"  # Same field - will be added to list
        requires: ["caption_shortening"] # Can link to multiple requirements to ensure their resulting dataset fields will expand.

  storage:
    data_dir: ./caption_data
    checkpoint_interval: 1000
    caption_buffer_size: 100
    job_buffer_size: 100
    contributor_buffer_size: 10
    checkpoint_dir: ./checkpoints

  auth:
    worker_tokens:
      - token: "worker-alpha-2024"
        name: "NVIDIA 4090 1"
      - token: "worker-beta-2024"
        name: "NVIDIA 4090 2"
      - token: "worker-kappa-2024"
        name: "NVIDIA 4090 3"
      - token: "example-worker-token"
        name: "Example Worker Token"
    monitor_tokens:
      - token: "letmein"
        name: "Default demo Monitor user"
    admin_tokens:
      - token: "admin-secret-2024"
        name: "Default admin user, misconfigured system"
